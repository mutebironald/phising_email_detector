{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "import pandas as pd;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "email_data = pd.read_csv('./sample_data/phis_email.csv')\n",
    "email_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nulls\n",
    "print(f'data info: {email_data.info()}');\n",
    "print(f'\\n duplicate rows: {email_data.duplicated().sum()}');\n",
    "\n",
    "print(f'\\n checking for null rows: {email_data.isnull().sum()}');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check  the email type distribution\n",
    "email_data['Email Type'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.drop(columns=['Unnamed: 0'], inplace=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_data.columns\n",
    "email_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 1 for safe email and 0 for Phising email\n",
    "email_data['Email Type'] = email_data['Email Type'].replace(['Safe Email','Phishing Email'],[1,0])\n",
    "email_data['Email Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "phishing_emails = email_data[email_data['Email Type'] == 0]\n",
    "non_phishing_emails = email_data[email_data['Email Type'] == 1]\n",
    "\n",
    "\n",
    "# Check for NaN values\n",
    "print(phishing_emails['Email Text'].isnull().sum())\n",
    "phishing_emails.loc[:, 'Email Text'] = phishing_emails['Email Text'].astype(str).fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate WordCloud for phishing emails\n",
    "wordcloud = WordCloud().generate(' '.join(phishing_emails['Email Text']))\n",
    "\n",
    "# Display the WordCloud\n",
    "plt.figure(figsize=(10, 6))  # Set figure size\n",
    "plt.imshow(wordcloud, interpolation='bilinear')  # Display the generated WordCloud\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file('./sample_data/phishing_wordcloud.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WordCloud for non-phishing emails\n",
    "non_phishing_wordcloud = WordCloud().generate(' '.join(non_phishing_emails['Email Text']))\n",
    "\n",
    "# Display the WordCloud for non-phishing emails\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(non_phishing_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file('./sample_data/safe_emails_wordcloud.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_data['text_length'] = email_data['Email Text'].apply(len)\n",
    "# email_data['text_length'].hist(bins=50)\n",
    "\n",
    "# Calculate email length\n",
    "email_data['email_length'] = email_data['Email Text'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Univariate visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Univariate Visualization: Histogram of Email Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(email_data['email_length'], bins=30, kde=True)  # KDE for smoothness\n",
    "plt.title('Distribution of Email Lengths')\n",
    "plt.xlabel('Email Length (Characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=email_data['email_length'].mean(), color='red', linestyle='--', label='Mean Email Length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate Visualization: Email Length vs. Word Count\n",
    "\n",
    "email_data['word_count'] = email_data['Email Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='email_length', y='word_count', hue='Email Type', data=email_data, alpha=0.7)\n",
    "plt.title('Email Length vs. Word Count by Email Type')\n",
    "plt.xlabel('Email Length (Characters)')\n",
    "plt.ylabel('Word Count')\n",
    "plt.axhline(y=email_data['word_count'].mean(), color='red', linestyle='--', label='Mean Word Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare your features and labels\n",
    "X = email_data['Email Text']\n",
    "y = email_data['Email Type']\n",
    "\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(y) \n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Limit to the top 5000 words\n",
    "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical ML Model: Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize the model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logistic = logistic_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logistic))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logistic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network: Feedforward Neural Network\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras import metrics\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
    "nn_model.add(Dense(64, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = (nn_model.predict(X_test_tfidf) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Neural Network Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
